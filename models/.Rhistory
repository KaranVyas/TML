strategy = "base"
path_proj = "/home/shared/1911TML/"
path_predictions = paste0(path_proj, "predictions/")
pathx = paste0(path_predictions, strategy, "/split_data/", learner, "/test/")
did_files = list.files(pathx)
names(did_files) = str_remove_all(did_files, "preds_did_|.csv")
pred_data = map_dfr(did_files, function(filx){
paste0(path_predictions, strategy, "/split_data/", learner, "/test/", filx) %>%
read_csv(col_types = cols(
molecule_id = col_character(),
fold = col_double(),
id = col_double(),
truth = col_double(),
response = col_double()
))
}, .id = "target_id")
pred_data = pred_data %>% mutate(sqerr = (truth - response)^2)
path_perf = paste0(path_proj, "performance/")
dir.create(path_perf, recursive = T)
pred_data %>% group_by(target_id, fold) %>%
summarise(rmse = sqrt(sum(sqerr)/n()),
truth_max = max(truth),
truth_min = min(truth),
nrmse = rmse / (truth_max - truth_min)) %>%
write_csv(paste0(path_perf, "perf_", strategy, "_", learner, ".csv"))
rm(list = ls())
path_proj_perf = "/home/shared/1911TML/performance/"
library(tidyverse)
perf_df = tibble(
fname = list.files(path_proj_perf)
) %>%
mutate(meta_learner = str_remove_all(fname, "perf_|.csv")) %>%
separate(meta_learner, into = c("meta_learner", "learner"), sep = "_", remove = F)
perf_data = map_dfr(perf_df$fname, ~ {
paste0(path_proj_perf, .x) %>% read_csv() %>%
mutate(fname = .x)
})
perf_data = inner_join(perf_df, perf_data)
# drop rows with NAs and infties
perf_data = perf_data %>% filter(truth_min>=0 & truth_max<15 & !is.na(nrmse) & !is.infinite(nrmse) & nrmse < 1 )
perf_data_sum = perf_data %>% group_by(meta_learner, learner, target_id) %>%
summarise(rmse_mean = mean(rmse, na.rm = T),
rmse_sd = sd(rmse,na.rm = T),
nrmse_mean = mean(nrmse, na.rm = T),
nrmse_sd = sd(nrmse,na.rm = T)) %>%
filter(!is.na(rmse_sd) & !is.na(nrmse_sd))
### BEGIN
# 15/02/21
# comparison XGB, to be removed when finsihed
chid_xgb <- perf_data_sum %>% filter(learner == "XGB") %>% pull(target_id)
perf_data_sum <- perf_data_sum %>% filter(target_id %in% chid_xgb)
View(perf_df)
# SUmmary of model performance
perf_data_sum %>% group_by(meta_learner, learner) %>%
summarise(rmse_Mean = mean(rmse_mean, na.rm = T),
rmse_SD = sd(rmse_mean,na.rm = T),
nrmse_Mean = mean(nrmse_mean, na.rm = T),
nrmse_SD = sd(nrmse_mean,na.rm = T)) %>% View()
rm(list = ls())
library(mlr)
library(tidyverse)
library(foreach)
library(doParallel)
set.seed(123, "L'Ecuyer")
learner_id = "XGB"
path_proj = "/home/shared/1911TML/"
path_datasets = paste0(path_proj, "datasets/transformed/",learner_id,"/")
path_splits = paste0(path_proj, "data_splits/")
path_models_full = paste0(path_proj, "models/transformed/full_data/",learner_id,"/")
dir.create(path_models_full, recursive = T)
path_models_split = paste0(path_proj, "models/transformed/split_data/",learner_id,"/")
dir.create(path_models_split, recursive = T)
path_predictions_train = paste0(path_proj, "predictions/transformed/split_data/",learner_id,"/train/")
dir.create(path_predictions_train, recursive = T)
path_predictions_test = paste0(path_proj, "predictions/transformed/split_data/",learner_id,"/test/")
dir.create(path_predictions_test, recursive = T)
data_info = read_csv(paste0(path_proj, "datasets_info.csv"))
# learner
parset <- makeParamSet(
makeDiscreteParam("nrounds", values = c(1000, 1500)),
makeDiscreteParam("eta", values = c(0.001, 0.01, 0.1, 0.2, 0.3))
)
rdesc <- makeResampleDesc("Holdout", split = 0.7)
lrn_full0 = makeLearner("regr.xgboost")
lrn_full0 = setHyperPars(lrn_full0,
par.vals = list(
max_depth = 6,
gamma = 0.05,
subsample = 0.5,
colsample_bytree = 1,
min_child_weight = 1
))
lrn_full <- makeTuneWrapper(lrn_full0,
resampling = rdesc,
measures = list(sse),
par.set = parset,
control = makeTuneControlGrid()
)
lrn_split0 = makeLearner("regr.xgboost")
lrn_split0 = setHyperPars(lrn_split0,
par.vals = list(
max_depth = 6,
gamma = 0.05,
subsample = 0.5,
colsample_bytree = 1,
min_child_weight = 1
))
lrn_split <- makeTuneWrapper(lrn_split0,
resampling = rdesc,
measures = list(sse),
par.set = parset,
control = makeTuneControlGrid()
)
dset_info$target_id[210:]
dset_info$target_id[210:2094]
data_info$target_id[210:2094]
R.version
rm(list = ls())
library(tidyverse)
learner = "XGB"
strategy = "transformed"
path_proj = "/home/shared/1911TML/"
path_predictions = paste0(path_proj, "predictions/")
pathx = paste0(path_predictions, strategy, "/split_data/", learner, "/test/")
did_files = list.files(pathx)
names(did_files) = str_remove_all(did_files, "preds_did_|.csv")
pred_data = map_dfr(did_files, function(filx){
paste0(path_predictions, strategy, "/split_data/", learner, "/test/", filx) %>%
read_csv(col_types = cols(
molecule_id = col_character(),
fold = col_double(),
id = col_double(),
truth = col_double(),
response = col_double()
))
}, .id = "target_id")
pred_data = pred_data %>% mutate(sqerr = (truth - response)^2)
path_perf = paste0(path_proj, "performance/")
dir.create(path_perf, recursive = T)
pred_data %>% group_by(target_id, fold) %>%
summarise(rmse = sqrt(sum(sqerr)/n()),
truth_max = max(truth),
truth_min = min(truth),
nrmse = rmse / (truth_max - truth_min)) %>%
write_csv(paste0(path_perf, "perf_", strategy, "_", learner, ".csv"))
rm(list = ls())
path_proj_perf = "/home/shared/1911TML/performance/"
library(tidyverse)
perf_df = tibble(
fname = list.files(path_proj_perf)
) %>%
mutate(meta_learner = str_remove_all(fname, "perf_|.csv")) %>%
separate(meta_learner, into = c("meta_learner", "learner"), sep = "_", remove = F)
perf_data = map_dfr(perf_df$fname, ~ {
paste0(path_proj_perf, .x) %>% read_csv() %>%
mutate(fname = .x)
})
perf_data = inner_join(perf_df, perf_data)
# drop rows with NAs and infties
perf_data = perf_data %>% filter(truth_min>=0 & truth_max<15 & !is.na(nrmse) & !is.infinite(nrmse) & nrmse < 1 )
perf_data_sum = perf_data %>% group_by(meta_learner, learner, target_id) %>%
summarise(rmse_mean = mean(rmse, na.rm = T),
rmse_sd = sd(rmse,na.rm = T),
nrmse_mean = mean(nrmse, na.rm = T),
nrmse_sd = sd(nrmse,na.rm = T)) %>%
filter(!is.na(rmse_sd) & !is.na(nrmse_sd))
# SUmmary of model performance
perf_data_sum %>% group_by(meta_learner, learner) %>%
summarise(rmse_Mean = mean(rmse_mean, na.rm = T),
rmse_SD = sd(rmse_mean,na.rm = T),
nrmse_Mean = mean(nrmse_mean, na.rm = T),
nrmse_SD = sd(nrmse_mean,na.rm = T)) %>% View()
rm(list = ls())
library(tidyverse)
learner = "XGB"
strategy = "stackednnls"
path_proj = "/home/shared/1911TML/"
path_predictions = paste0(path_proj, "predictions/")
pathx = paste0(path_predictions, strategy, "/split_data/", learner, "/test/")
did_files = list.files(pathx)
names(did_files) = str_remove_all(did_files, "preds_did_|.csv")
pred_data = map_dfr(did_files, function(filx){
paste0(path_predictions, strategy, "/split_data/", learner, "/test/", filx) %>%
read_csv(col_types = cols(
molecule_id = col_character(),
fold = col_double(),
id = col_double(),
truth = col_double(),
response = col_double()
))
}, .id = "target_id")
pred_data = pred_data %>% mutate(sqerr = (truth - response)^2)
path_perf = paste0(path_proj, "performance/")
dir.create(path_perf, recursive = T)
pred_data %>% group_by(target_id, fold) %>%
summarise(rmse = sqrt(sum(sqerr)/n()),
truth_max = max(truth),
truth_min = min(truth),
nrmse = rmse / (truth_max - truth_min)) %>%
write_csv(paste0(path_perf, "perf_", strategy, "_", learner, ".csv"))
rm(list = ls())
library(tidyverse)
learner = "XGB"
strategy = "stackedridge"
path_proj = "/home/shared/1911TML/"
path_predictions = paste0(path_proj, "predictions/")
pathx = paste0(path_predictions, strategy, "/split_data/", learner, "/test/")
did_files = list.files(pathx)
names(did_files) = str_remove_all(did_files, "preds_did_|.csv")
pred_data = map_dfr(did_files, function(filx){
paste0(path_predictions, strategy, "/split_data/", learner, "/test/", filx) %>%
read_csv(col_types = cols(
molecule_id = col_character(),
fold = col_double(),
id = col_double(),
truth = col_double(),
response = col_double()
))
}, .id = "target_id")
pred_data = pred_data %>% mutate(sqerr = (truth - response)^2)
path_perf = paste0(path_proj, "performance/")
dir.create(path_perf, recursive = T)
pred_data %>% group_by(target_id, fold) %>%
summarise(rmse = sqrt(sum(sqerr)/n()),
truth_max = max(truth),
truth_min = min(truth),
nrmse = rmse / (truth_max - truth_min)) %>%
write_csv(paste0(path_perf, "perf_", strategy, "_", learner, ".csv"))
rm(list = ls())
path_proj_perf = "/home/shared/1911TML/performance/"
library(tidyverse)
perf_df = tibble(
fname = list.files(path_proj_perf)
) %>%
mutate(meta_learner = str_remove_all(fname, "perf_|.csv")) %>%
separate(meta_learner, into = c("meta_learner", "learner"), sep = "_", remove = F)
perf_data = map_dfr(perf_df$fname, ~ {
paste0(path_proj_perf, .x) %>% read_csv() %>%
mutate(fname = .x)
})
perf_data = inner_join(perf_df, perf_data)
# drop rows with NAs and infties
perf_data = perf_data %>% filter(truth_min>=0 & truth_max<15 & !is.na(nrmse) & !is.infinite(nrmse) & nrmse < 1 )
perf_data_sum = perf_data %>% group_by(meta_learner, learner, target_id) %>%
summarise(rmse_mean = mean(rmse, na.rm = T),
rmse_sd = sd(rmse,na.rm = T),
nrmse_mean = mean(nrmse, na.rm = T),
nrmse_sd = sd(nrmse,na.rm = T)) %>%
filter(!is.na(rmse_sd) & !is.na(nrmse_sd))
# SUmmary of model performance
perf_data_sum %>% group_by(meta_learner, learner) %>%
summarise(rmse_Mean = mean(rmse_mean, na.rm = T),
rmse_SD = sd(rmse_mean,na.rm = T),
nrmse_Mean = mean(nrmse_mean, na.rm = T),
nrmse_SD = sd(nrmse_mean,na.rm = T)) %>% View()
rm(list = ls())
library(mlr)
library(tidyverse)
set.seed(123)
path_proj = "/home/shared/openml_stdy07/"
file_datasets = paste0(path_proj, "data/original_datasets.csv")
file_splits = paste0(path_proj, "data_splits/datasets_splits.csv")
dsets = inner_join(
read_csv(file_splits),
read_csv(file_datasets),
by = c("rows_id" = "openml_task", "flow")
)
#-----------Issues
dsets = dsets %>% filter(flow != "weka.ZeroR")
dsets = split(dsets, dsets$flow)
tasks = imap(dsets, ~ {
.x %>% select(-c(flow, rows_id, fold)) %>% as.data.frame() %>%
makeRegrTask(id = .y, data = ., target = "area_under_roc_curve")
})
names(tasks) = names(dsets)
subsets = map(dsets, ~ .x %>% select(flow, rows_id, fold))
names(subsets) = names(dsets)
path_predictions = paste0(path_proj, "predictions/")
dir.create(path_predictions, recursive = T)
# learner
# learner
parset <- makeParamSet(
makeDiscreteParam("nrounds", values = c(1000, 1500)),
makeDiscreteParam("eta", values = c(0.001, 0.01, 0.1, 0.2, 0.3))
)
rdesc <- makeResampleDesc("Holdout", split = 0.7)
lrn_full0 = makeLearner("regr.xgboost")
lrn_full0 = setHyperPars(lrn_full0,
par.vals = list(
max_depth = 6,
gamma = 0.05,
subsample = 0.5,
colsample_bytree = 1,
min_child_weight = 1
))
lrn_full <- makeTuneWrapper(lrn_full0,
resampling = rdesc,
measures = list(sse),
par.set = parset,
control = makeTuneControlGrid()
)
learners = list(XGB = lrn_full)
cv_train = function(taskx, subsetx, lrnx, .folds = 10) {
mdls = map(1:.folds, ~{
subs_trn = which(subsetx$fold != .x)
train(learner = lrnx, task = taskx, subset = subs_trn)
})
}
cv_pred = function(taskx, subsetx, mdlx, .test = TRUE, .folds = 10) {
cat("task = ", taskx$task.desc$id, "\n")
preds = map_dfr(1:.folds, ~{
if(.test) subs = which(subsetx$fold == .x)
else subs = which(subsetx$fold != .x)
pred = (predict(mdlx[[.x]], task = taskx, subset = subs))$data
pred %>% mutate(fold = .x)
})
preds %>% mutate(rows_id = subsetx$rows_id[preds$id])
}
cat("\nCreating the models...")
tmp = foreach(lrnx = learners, lrn_id = names(learners),
.packages = c("mlr","dplyr", "purrr"),
.errorhandling = "remove",
.options.multicore=list(preschedule=FALSE)) %do% {     #DOPAR!!!
cat("learner = ", lrn_id, "...")
mdls = map2(tasks, subsets, cv_train, lrnx = lrnx)
preds_all_trn = pmap_dfr(list(tasks, subsets, mdls), cv_pred, .test = F, .id = "flow")
write_csv(preds_all_trn, paste0(path_predictions, "split_train_base_", lrn_id, ".csv"))
preds_all_tst = pmap_dfr(list(tasks, subsets, mdls), cv_pred, .test = T, .id = "flow")
write_csv(preds_all_tst, paste0(path_predictions, "split_test_base_", lrn_id, ".csv"))
cat("DONE\n")
TRUE
}
library(foreach)
tmp = foreach(lrnx = learners, lrn_id = names(learners),
.packages = c("mlr","dplyr", "purrr"),
.errorhandling = "remove",
.options.multicore=list(preschedule=FALSE)) %do% {     #DOPAR!!!
cat("learner = ", lrn_id, "...")
mdls = map2(tasks, subsets, cv_train, lrnx = lrnx)
preds_all_trn = pmap_dfr(list(tasks, subsets, mdls), cv_pred, .test = F, .id = "flow")
write_csv(preds_all_trn, paste0(path_predictions, "split_train_base_", lrn_id, ".csv"))
preds_all_tst = pmap_dfr(list(tasks, subsets, mdls), cv_pred, .test = T, .id = "flow")
write_csv(preds_all_tst, paste0(path_predictions, "split_test_base_", lrn_id, ".csv"))
cat("DONE\n")
TRUE
}
rm(list = ls())
library(mlr)
library(tidyverse)
set.seed(123)
path_proj = "/home/shared/openml_stdy07/"
path_predictions = paste0(path_proj, "predictions/")
# learner
#learners_id = c("RF", "SVM", "KNN", "Ridge")
learners_id = "XGB"
#walk(learners_id, ~{
preds = read_csv(paste0(path_predictions, "full_base_", .x, ".csv"))
rm(list = ls())
library(mlr)
library(tidyverse)
set.seed(123)
path_proj = "/home/shared/openml_stdy07/"
path_predictions = paste0(path_proj, "predictions/")
# learner
#learners_id = c("RF", "SVM", "KNN", "Ridge")
learners_id = "XGB"
#walk(learners_id, ~{
preds = read_csv(paste0(path_predictions, "full_base_", learners_id, ".csv"))
preds = preds %>% split(.,.$flow)
preds = map(preds, function(predx) {
predx %>% spread(key = flow_col, value = response)
})
.x = learners_id
write_rds(preds, paste0(path_proj, "data/transformed_datasets_",.x, ".rds"))
rm(list = ls())
library(tidyverse)
library(glmnet)
algorithms = c("XGB")
meta = c("base", "transformed")
names(meta) = meta
path_project = "/home/shared/openml_stdy07/"
path_predictions = paste0(path_project, "predictions/")
get_data = function(algorithm, subset = "test") {
map_dfr(meta,
~ read_csv(
paste0(path_predictions, "split_", subset, "_", .x, "_", algorithm, ".csv"),
col_types = cols(
flow = col_character(),
id = col_double(),
truth = col_double(),
response = col_double(),
fold = col_double(),
rows_id = col_character()
)
), .id = "meta_alg") %>%
spread(key = "meta_alg", value = "response") %>%
split(., .$flow)
}
pred_model = function(train_df, test_df) {
cat("dataset = ", train_df$flow[1], "\n")
train_df = train_df %>% split(., .$fold)
test_df = test_df %>% split(., .$fold)
map2_dfr(train_df, test_df, ~{
x = .x %>%  select(c(base, transformed)) %>% as.matrix()
y = .x %>%  select(truth) %>% as.matrix()
mdl = glmnet(x,y, lambda=0, lower.limits=0, intercept=FALSE)
newx = .y %>%  select(c(base, transformed)) %>% as.matrix()
.y %>% mutate(response = predict(mdl, newx = newx) %>% as.vector)
})
}
walk(algorithms, function(algox){
train_lst = get_data(algorithm = algox, subset = "train")
test_lst = get_data(algorithm = algox, subset = "test")
preds = map2_dfr(train_lst, test_lst, pred_model)
preds %>%
select(-c(base, transformed)) %>%
write_csv(paste0(path_predictions, "split_test_nnls_", algox, ".csv"))
})
rm(list = ls())
library(tidyverse)
library(mlr)
algorithms = c("XGB")
meta = c("base", "transformed")
names(meta) = meta
path_project = "/home/shared/openml_stdy07/"
path_predictions = paste0(path_project, "predictions/")
lrn = makeLearner("regr.cvglmnet")
lrn = setHyperPars(lrn, alpha = 0, nfolds = 3)
get_data = function(algorithm, subset = "test") {
map_dfr(meta,
~ read_csv(
paste0(path_predictions, "split_", subset, "_", .x, "_", algorithm, ".csv"),
col_types = cols(
flow = col_character(),
id = col_double(),
truth = col_double(),
response = col_double(),
fold = col_double(),
rows_id = col_character()
)
), .id = "meta_alg") %>%
spread(key = "meta_alg", value = "response") %>%
split(., .$flow)
}
pred_model = function(train_df, test_df) {
cat("dataset = ", train_df$flow[1], "\n")
train_df = train_df %>% split(., .$fold)
test_df = test_df %>% split(., .$fold)
map2_dfr(train_df, test_df, ~{
tsk = makeRegrTask(data = (.x %>% select(base, transformed, truth) %>% as.data.frame()), target = "truth")
mdl = train(lrn, tsk)
newx = .y %>%  select(c(base, transformed)) %>% as.data.frame()
.y %>% mutate(response = predict(mdl, newdata = newx)$data$response)
})
}
walk(algorithms, function(algox){
train_lst = get_data(algorithm = algox, subset = "train")
test_lst = get_data(algorithm = algox, subset = "test")
preds = map2_dfr(train_lst, test_lst, pred_model)
preds %>%
select(-c(base, transformed)) %>%
write_csv(paste0(path_predictions, "split_test_stackedridge_", algox, ".csv"))
})
rm(list = ls())
library(tidyverse)
algorithms = c("RF", "SVM", "KNN", "DNN", "XGB") #, "Ridge")
meta = c("base", "transformed", "stackedridge", "nnls")
path_project = "/home/shared/openml_stdy07/"
path_predictions = paste0(path_project, "predictions/")
prefix_fnam = "split_test_"
file_names = list.files(path_predictions, pattern = prefix_fnam)
perf_df = tibble(filename = file_names) %>%
mutate(meta_alg = str_remove_all(filename, paste0(prefix_fnam,"|.csv"))) %>%
separate(meta_alg, into = c("meta_alg", "algorithm")) %>%
filter(meta_alg %in% meta, algorithm %in% algorithms)
# Load predictions file
perf_df = map_dfr(perf_df$filename, ~ {
read_csv(paste0(path_predictions, .x), col_types = cols(
flow = col_character(),
id = col_double(),
truth = col_double(),
response = col_double(),
fold = col_double(),
rows_id = col_character()
)) %>%
mutate(filename = .x)
}) %>%
inner_join(perf_df)
head(perf_df)
# prediction error
perf_df = perf_df %>% mutate(err = (truth - response)^2)
# RMSE and NRMSE / fold
perf_df = perf_df %>% group_by(meta_alg, algorithm, flow, fold) %>%
summarise(
rmse = sqrt(sum(err)/n()),
truth_max = max(truth),
truth_min = min(truth),
nrmse = rmse/(truth_max-truth_min)
) %>%
group_by(meta_alg, algorithm, flow) %>%
summarise(
rmse = mean(rmse),
nrmse = mean(nrmse)
)
ggplot(perf_df, aes(x = algorithm, y = rmse, colour = meta_alg)) + geom_boxplot()
ggplot(perf_df, aes(x = algorithm, y = nrmse, colour = meta_alg)) + geom_boxplot()
# SUmmary of model performance
perf_df %>% group_by(meta_alg, algorithm) %>%
summarise(rmse_Mean = mean(rmse, na.rm = T),
rmse_SD = sd(rmse,na.rm = T),
nrmse_Mean = mean(nrmse, na.rm = T),
nrmse_SD = sd(nrmse,na.rm = T)) %>% View()
